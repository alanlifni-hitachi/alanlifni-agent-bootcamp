"""Multi-agent Plan-and-Execute workflow via OpenAI Agents SDK.

Note: this implementation does not unlock the full potential and flexibility
of LLM agents. Use this reference implementation only if your use case requires
the additional structures, and you are okay with the additional complexities.

Log traces to LangFuse for observability and evaluation.
"""

import asyncio
from typing import Any, AsyncGenerator

import agents
import gradio as gr
from dotenv import load_dotenv
from gradio.components.chatbot import ChatMessage
from langfuse import propagate_attributes
from pydantic import BaseModel

from src.utils import (
    oai_agent_items_to_gradio_messages,
    pretty_print,
    setup_langfuse_tracer,
)
from src.utils.agent_session import get_or_create_session
from src.utils.client_manager import AsyncClientManager
from src.utils.gradio import COMMON_GRADIO_CONFIG
from src.utils.langfuse.shared_client import langfuse_client
from src.utils.logging import set_up_logging


PLANNER_INSTRUCTIONS = """\
You are a research planner. \
Given a user's query, produce a list of search terms that can be used to retrieve
relevant information from a knowledge base to answer the question. \
As you are not able to clarify from the user what they are looking for, \
your search terms should be broad and cover various aspects of the query. \
Output up to 10 search terms to query the knowledge base. \
Note that the knowledge base is a Wikipedia dump and cuts off at May 2025.
"""

RESEARCHER_INSTRUCTIONS = """\
You are a research assistant with access to a knowledge base. \
Given a potentially broad search term, your task is to use the search tool to \
retrieve relevant information from the knowledge base and produce a short \
summary of at most 300 words. You must pass the initial search term directly to \
the search tool without any modifications and, only if necessary, refine your \
search based on the results you get back. Your summary must be based solely on \
a synthesis of all the search results and should not include any information that \
is not present in the search results. For every fact you include in the summary, \
ALWAYS include a citation both in-line and at the end of the summary as a numbered \
list. The citation at the end should include relevant metadata from the search \
results. Do NOT return raw search results.
"""

WRITER_INSTRUCTIONS = """\
You are an expert at synthesizing information and writing coherent reports. \
Given a user's query and a set of search summaries, synthesize these into a \
coherent report that answers the user's question. The length of the report should be \
proportional to the complexity of the question. For queries that are more complex, \
ensure that the report is well-structured, with clear sections and headings where \
appropriate. Make sure to use the citations from the search summaries to back up \
any factual claims you make. \
Do not make up any information outside of the search summaries.
"""


class SearchItem(BaseModel):
    """A single search item in the search plan."""

    # The search term to be used in the knowledge base search
    search_term: str

    # A description of the search term and its relevance to the query
    reasoning: str


class SearchPlan(BaseModel):
    """A search plan containing multiple search items."""

    search_steps: list[SearchItem]

    def __str__(self) -> str:
        """Return a string representation of the search plan."""
        return "\n".join(
            f"Search Term: {step.search_term}\nReasoning: {step.reasoning}\n"
            for step in self.search_steps
        )


class ResearchReport(BaseModel):
    """Model for the final report generated by the writer agent."""

    # The summary of the research findings
    summary: str

    # full report text
    full_report: str


async def _create_search_plan(
    planner_agent: agents.Agent, query: str, session: agents.Session | None = None
) -> SearchPlan:
    """Create a search plan using the planner agent."""
    response = await agents.Runner.run(planner_agent, input=query, session=session)
    return response.final_output_as(SearchPlan)


async def _generate_final_report(
    writer_agent: agents.Agent,
    search_results: list[str],
    query: str,
    session: agents.Session | None = None,
) -> agents.RunResult:
    """Generate the final report using the writer agent."""
    input_data = f"Original question: {query}\n"
    input_data += "Search summaries:\n" + "\n".join(
        f"{i + 1}. {result}" for i, result in enumerate(search_results)
    )

    with langfuse_client.start_as_current_observation(
        name="Writer-Agent", as_type="chain", input=input_data
    ) as obs:
        response = await agents.Runner.run(
            writer_agent, input=input_data, session=session
        )
        obs.update(output=response.final_output)
        return response


async def _main(
    query: str, history: list[ChatMessage], session_state: dict[str, Any]
) -> AsyncGenerator[list[ChatMessage], Any]:
    """Run multi-agent planner-researcher setup."""
    # Initialize list of chat messages for a single turn
    turn_messages: list[ChatMessage] = []

    # Construct an in-memory SQLite session for the agent to maintain
    # conversation history across multiple turns of a chat
    # This makes it possible to ask follow-up questions that refer to
    # previous turns in the conversation
    session = get_or_create_session(history, session_state)

    with (
        langfuse_client.start_as_current_observation(
            name="Plan-and-Execute-Workflow", as_type="chain", input=query
        ) as obs,
        propagate_attributes(
            session_id=session.session_id  # Propagate session_id to all child observations
        ),
    ):
        # Create a search plan
        with langfuse_client.start_as_current_observation(
            name="Planner-Agent", as_type="chain", input=query
        ) as planner_obs:
            search_plan = await _create_search_plan(
                planner_agent, query, session=session
            )
            planner_obs.update(output=str(search_plan))

        turn_messages.append(
            ChatMessage(
                role="assistant",
                content="",
                metadata={"title": "**Search Plan**", "id": "search-plan"},
            )
        )
        for step in search_plan.search_steps:
            turn_messages.append(
                ChatMessage(
                    role="assistant",
                    content=(f"_Reasoning:_ {step.reasoning}"),
                    metadata={
                        "title": f"**Search Term:** {step.search_term}",
                        "parent_id": "search-plan",
                        "status": "done",  # This makes it collapsed by default
                    },
                )
            )
        pretty_print(turn_messages)
        yield turn_messages

        # Execute the search plan
        # NOTE: searches are done sequentially here for simplicity.
        # TODO: As an exercise, try to paralleize the execution of the search steps.
        search_results = []
        for step in search_plan.search_steps:
            with langfuse_client.start_as_current_observation(
                name="Researcher-Agent", as_type="chain", input=step.search_term
            ) as researcher_obs:
                response = await agents.Runner.run(
                    research_agent,
                    input=step.search_term,
                    session=session,
                    max_turns=30,  # Allow more turns for complex searches
                )
                search_result: str = response.final_output
                researcher_obs.update(output=search_result)

            search_results.append(search_result)
            turn_messages += oai_agent_items_to_gradio_messages(
                response.new_items, is_final_output=False
            )
            yield turn_messages

        # Generate the final report
        writer_agent_response = await _generate_final_report(
            writer_agent, search_results, query, session=session
        )

        report = writer_agent_response.final_output_as(ResearchReport)
        obs.update(output=report)
        turn_messages.append(
            ChatMessage(
                role="assistant",
                content=f"## Summary\n{report.summary}\n\n## Full Report\n{report.full_report}",
            )
        )
        pretty_print(turn_messages)
        yield turn_messages


if __name__ == "__main__":
    load_dotenv(verbose=True)

    # Set logging level and suppress some noisy logs from dependencies
    set_up_logging()

    # Set up LangFuse for tracing
    setup_langfuse_tracer()

    # Initialize client manager
    # This class initializes the OpenAI and Weaviate async clients, as well as the
    # Weaviate knowledge base tool. The initialization is done once when the clients
    # are first accessed, and the clients are reused for subsequent calls.
    client_manager = AsyncClientManager()

    # Use smaller, faster model for focused search tasks
    worker_model = client_manager.configs.default_worker_model
    # Use larger, more capable model for complex planning and reasoning
    planner_model = client_manager.configs.default_planner_model

    planner_agent = agents.Agent(
        name="Planner Agent",
        instructions=PLANNER_INSTRUCTIONS,
        model=agents.OpenAIChatCompletionsModel(
            model=planner_model,
            openai_client=client_manager.openai_client,
        ),
        output_type=SearchPlan,
    )

    research_agent = agents.Agent(
        name="Research Agent",
        instructions=RESEARCHER_INSTRUCTIONS,
        tools=[agents.function_tool(client_manager.knowledgebase.search_knowledgebase)],
        model=agents.OpenAIChatCompletionsModel(
            model=worker_model,
            openai_client=client_manager.openai_client,
        ),
        # Force the agent to use the search tool for every query
        model_settings=agents.ModelSettings(tool_choice="required"),
    )

    writer_agent = agents.Agent(
        name="Writer Agent",
        instructions=WRITER_INSTRUCTIONS,
        model=agents.OpenAIChatCompletionsModel(
            model=planner_model,  # Stronger model for complex synthesis
            openai_client=client_manager.openai_client,
        ),
        output_type=ResearchReport,
    )

    demo = gr.ChatInterface(
        _main,
        **COMMON_GRADIO_CONFIG,
        examples=[
            [
                "Write a structured report on the history of AI, covering: "
                "1) the start in the 50s, 2) the first AI winter, 3) the second AI winter, "
                "4) the modern AI boom, 5) the evolution of AI hardware, and "
                "6) the societal impacts of modern AI"
            ],
            [
                "Compare the box office performance of 'Oppenheimer' with the third Avatar movie"
            ],
        ],
        title="2.2.1: Plan-and-Execute Multi-Agent System for Retrieval-Augmented Generation",
    )

    try:
        demo.launch(share=True)
    finally:
        asyncio.run(client_manager.close())
